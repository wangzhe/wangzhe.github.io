<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>深度学习顺序模型第三周 | 杰克船长的小屋</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="来到最后一周了，本周分为10课，两个大部分 Various sequence to sequence architectures这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。 Basic Models12x: Jane visite I&#39;Afrique en septembrey: Jane is visiting">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习顺序模型第三周">
<meta property="og:url" content="http://wangzhe.github.io/2018/09/13/deep-learning-sequence-models-w3/index.html">
<meta property="og:site_name" content="杰克船长的小屋">
<meta property="og:description" content="来到最后一周了，本周分为10课，两个大部分 Various sequence to sequence architectures这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。 Basic Models12x: Jane visite I&#39;Afrique en septembrey: Jane is visiting">
<meta property="og:locale">
<meta property="og:image" content="http://wangzhe.github.io/2018/09/13/deep-learning-sequence-models-w3/error_analysis.png">
<meta property="article:published_time" content="2018-09-13T08:07:05.000Z">
<meta property="article:modified_time" content="2021-01-04T05:38:30.276Z">
<meta property="article:author" content="Jack Wang">
<meta property="article:tag" content="tech">
<meta property="article:tag" content="deeplearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangzhe.github.io/2018/09/13/deep-learning-sequence-models-w3/error_analysis.png">
  
    <link rel="alternative" href="/atom.xml" title="杰克船长的小屋" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
<link rel="stylesheet" href="/css/style.css">

  

<meta name="generator" content="Hexo 5.3.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">杰克船长的小屋</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://wangzhe.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-deep-learning-sequence-models-w3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/13/deep-learning-sequence-models-w3/" class="article-date">
  <time datetime="2018-09-13T08:07:05.000Z" itemprop="datePublished">Sep 13 2018</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Technology/">Technology</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习顺序模型第三周
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>来到最后一周了，本周分为10课，两个大部分</p>
<h1 id="Various-sequence-to-sequence-architectures"><a href="#Various-sequence-to-sequence-architectures" class="headerlink" title="Various sequence to sequence architectures"></a>Various sequence to sequence architectures</h1><p>这个部分是本周的重点，8个课都是围绕着展开的。这种模型被广泛应用在了文字翻译和语义识别领域。让我们来看看都讲述了些什么。</p>
<h2 id="Basic-Models"><a href="#Basic-Models" class="headerlink" title="Basic Models"></a>Basic Models</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x: Jane visite I&#39;Afrique en septembre</span><br><span class="line">y: Jane is visiting Africa in September</span><br></pre></td></tr></table></figure>
<p>这里用了两套模型进行了组合，</p>
<ul>
<li>作为encoding network：$x^{<1>}, x^{<2>}, x^{<3>}, x^{<4>} … x^{ &lt; T_x &gt; }$，可以是一个RNN模型（GRU或者LSTM）</li>
<li>作为decoding network：$y^{<1>}, y^{<2>}, y^{<3>}, y^{<4>} … y^{ &lt; T_y &gt; }$，同时后面继续跟随了</li>
</ul>
<p>这样一种算法，同样对于Image Captioning这样的应用有效。应用是我们的输入是一张图片，应用的输出会把图片转化成一个语义</p>
<script type="math/tex; mode=display">\begin {matrix}
     y^{<1>} & y^{<2>} & y^{<3>} & y^{<4>} & y^{<5>} & y^{<6>} \\
     A & Cat & sitting & on & a & chair \\
\end {matrix}</script><p>在这样一个应用场景下，我们可以用CNN模型作为encoding network，把这个的输出，feed到RNN的顺序模型里面，进行训练。这个的输出，就是一个对Image的描述了。</p>
<PlaceHolder for image>

<h2 id="Picking-the-most-likely-sentence"><a href="#Picking-the-most-likely-sentence" class="headerlink" title="Picking the most likely sentence"></a>Picking the most likely sentence</h2><h3 id="Machine-translation-as-building-a-conditional-language-model"><a href="#Machine-translation-as-building-a-conditional-language-model" class="headerlink" title="Machine translation as building a conditional language model"></a>Machine translation as building a conditional language model</h3><p>一个正常的language model，是一个从$a^{0}$ 到 $x^{<1>}$ 再到 $\hat y^{<1>} $ 再进行混合的演进的顺序模型。同时在这个模型里，会有 $ P(y^{<1>}, …, y^{&lt; T_x &gt;})$ 的数值，来代表每一个过程中产生的y的可能性。这个一般是用来生成普通的句子</p>
<p>Machine Translation Model对应的是两个部分，encoding model和decoding model。可以看到decoding model和language model是很相似的。做为encoding model的输出，就是decoding model的输入 $ a_{<0>}$</p>
<p>这也是为什么我们把Machine translation model也叫做conditional language model。在形象一点的说法是，一段被翻译后的句子（比如Jane is visiting Africa）本身，是由前一个被翻译前句子的作为条件形成的。这个说法还挺有意思的，让我认清了翻译这件事情的本质。的确是这样。</p>
<h3 id="Finding-the-most-likely-translation-（如何找到最好翻译）"><a href="#Finding-the-most-likely-translation-（如何找到最好翻译）" class="headerlink" title="Finding the most likely translation （如何找到最好翻译）"></a>Finding the most likely translation （如何找到最好翻译）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x: Jane visite I&#39;Afrique en septembre</span><br></pre></td></tr></table></figure>
<p>于是乎，我们就有了 $ P(y^{<1>}, …, y^{&lt; T_x &gt;} |x) $ 这样一个可能性的表示。这里的x就是上面说的那段法语句子。所以整个翻译模型的结果就是寻找P的最大值，找到最有可能的translation结果</p>
<h3 id="Why-not-Greedy-Search"><a href="#Why-not-Greedy-Search" class="headerlink" title="Why not Greedy Search"></a>Why not Greedy Search</h3><p>Greedy Search是一种算法，大概的意思就是，因为有了x(翻译前句子的输入)，我们接下来寻找每一个 $ P(y^{<1>} |x)$, 然后是$ P(y^{<2>} |x y^{<1>})$，一次类推，就是每次都寻找那个最合适的。而不是 $ P(y|x)$。我觉得其实这里不用多解释，这就是一个局部优化和整体优化的问题。如果计算量允许，肯定尽量采用整体优化，这样才能取得好结果。</p>
<p>既然Greedy Search不好用，那么如何从$ 10000^{10} $ (假设vocabulary有10000个，句子有10个单次) 这么多种可能性里进行选取呢？当然第一方法，就是使用x作为输入，寻找较大可能性。那么然后呢？还有什么方法？于是这里就用到了更进一步的Search Algorithm。这就又回到了这一课的主题，就是寻找最好的翻译，而不是随机寻找翻译结果（这里作者列举的例子是going和visiting，前者更加常见，但是后者更加适合Jane的语境）</p>
<h2 id="Beam-Search-Algorith"><a href="#Beam-Search-Algorith" class="headerlink" title="Beam Search Algorith"></a>Beam Search Algorith</h2><p>关于Beam Search算法，第一步是吧10000 words的vacabulary放进array里面。</p>
<script type="math/tex; mode=display">\begin {pmatrix}
     a\\
     \vdots\\
     in\\
     \vdots\\
     jane\\
     \vdots\\
     september\\
     \vdots\\
     zulu\\
\end {pmatrix}</script><p>所以这里的Step1，第一步就是 $P(y^{<1>}|x)$ 对应的单词。这里插入一个概念B，叫做Beam width, 例如B=3.这里给出的Beam的数值，就是为了系统记录下B个最有可能的单词。看起来，其实意思就是说，Greedy Search不是每次只找一个最好的么，这个Beam Search是根据Beam的数值，找B个最好的。</p>
<p>接下来进行Step2第二步。就很简单了，其实就是因为前一个假设是in了，那想标准的language model一样，会有一个$P(y^{<2>}|x”in“)$这样的表达式，来表示下一个最大可能的单词。因此下面这个公式还挺重要的，就是：</p>
<script type="math/tex; mode=display">P(y^{<1>}, y^{<2>} |x) = P(y^{<1>}|x) \times P(y^{<2>}|x”in“)</script><p>Again, 这里因为Beam Width为3，所以我们还是，选择最大可能的三个，不过这回就是三个pair了 $P(y^{<1>}, y^{<2>} |x) $. 如此循环进行下一步的运算。你看，这里B=1的话，那就真的是Greedy Search了</p>
<h2 id="Refinements-to-Beam-Search"><a href="#Refinements-to-Beam-Search" class="headerlink" title="Refinements to Beam Search"></a>Refinements to Beam Search</h2><h3 id="Length-normalization"><a href="#Length-normalization" class="headerlink" title="Length normalization"></a>Length normalization</h3><p>这个就是Beam Search里面的一个部分。他的用法是这样的。</p>
<p>如同前文讲的，Beam Search的核心是寻找到最大的B个可能性，并进行模型推演。所以</p>
<script type="math/tex; mode=display">P(y|x) = avg max \prod_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>但是这里有一个问题，就是若干个百分之几十的数字相乘，会让这个结果趋近于非常小，不方便计算。于是这里引入了log，作为计算函数。这里P的数值越大，</p>
<script type="math/tex; mode=display">\log^{P(y|x)} = avg max \sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><script type="math/tex; mode=display">\frac{1}{Ty^\alpha} avg max \sum_{t=1}^{T_y} P(y^{<t>} | x, y^{<1>}, y^{<2>}, ... , y^{<t-1>} )</script><p>$ \alpha $ 一般是从0到1的一个中间值，比如0.7。这个数用来对于模型进行校正和调整是比较管用的</p>
<h2 id="Error-Analysis-on-Beam-Search"><a href="#Error-Analysis-on-Beam-Search" class="headerlink" title="Error Analysis on Beam Search"></a>Error Analysis on Beam Search</h2><p>关于整个翻译过程实际上是用到了Beam Search算法和两个RNN模型，对于结果而言，如何评价到底是哪里不好导致的问题。Error Analysis提供了一些方法</p>
<img src="/2018/09/13/deep-learning-sequence-models-w3/error_analysis.png" class="" title="[Error Analysis]">
<p>Case1: $ P(y_{<em>}|x) &gt; P( \hat y|x) $<br>在这种情况下，因为Beam Search的作用是用来进行选取$\hat y$，那么既然命名$P(y_{</em>}|x)$会更好，但是Beam Search却选的不对。说明Beam Search有问题</p>
<p>Case2：$ P(y_{*}|x) &lt; P( \hat y|x) $<br>RNN负责预测 $\hat y$ 而他错误的将$ P( \hat y|x) $ 生成了一个更大的P数值，这说明生成有问题。是RNN需要被调整</p>
<h2 id="Attention-Module-intuition"><a href="#Attention-Module-intuition" class="headerlink" title="Attention Module intuition"></a>Attention Module intuition</h2><p>对比于简单使用输入RNN和输出RNN，Attention Module更好的处理翻译过程中的长句子问题。我们注意到在日常翻译中，MT在处理10个左右单词的句子时候，表现效果会比较好。但是如果再长，效果就会下降的非常明显。因此我们需要更好的模型来改进这一点。</p>
<p>首先分析一下原因，主要是单词的记忆导致的。事实上，人类在翻译长句子的时候，采用的是一部分一部分的翻译方式，并不会把他们都记录下来，因为记忆的原因。这个事情同样出现在机器上面，因为计算资源有限，所以我们也无法给计算机留出那么大量的选项进行综合运算。</p>
<ul>
<li><p>Attention Weight，标注了对于一个生成的词来讲，哪些input word是应该被关注的，以及关注Weight是多少</p>
</li>
<li><p>$ \alpha^{ <1,2> } $ 这个表示第一个翻译后词汇，需要对翻译前句子中的第二个词的关注度有多高</p>
</li>
</ul>
<h2 id="Atention-Module"><a href="#Atention-Module" class="headerlink" title="Atention Module"></a>Atention Module</h2><script type="math/tex; mode=display">\sum_{t} \alpha^{ <1, t> } = 1</script><script type="math/tex; mode=display">C^{ <1> } （context）=  \sum_{t} \alpha^{ <1, t> } a^{ <t> }</script><script type="math/tex; mode=display">bug_{a} =  \alpha * sprint_{a} + \beta * sprint_{a-1} + bias</script><h1 id="领域应用"><a href="#领域应用" class="headerlink" title="领域应用"></a>领域应用</h1><h2 id="语音识别-（Speech-Recorgnition）"><a href="#语音识别-（Speech-Recorgnition）" class="headerlink" title="语音识别 （Speech Recorgnition）"></a>语音识别 （Speech Recorgnition）</h2><p>Audio Clip x，translate to ，Transcript y </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://wangzhe.github.io/2018/09/13/deep-learning-sequence-models-w3/" data-id="ckjjlgj57000s1oz46y010o67" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deeplearning/" rel="tag">deeplearning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tech/" rel="tag">tech</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/01/02/aliyun-server-less/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          阿里云函数计算
        
      </div>
    </a>
  
  
    <a href="/2018/09/08/deep-learning-sequence-models-w2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">深度学习顺序模型第二周</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Business/">Business</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Business-Strategy/">Business Strategy</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Diary/">Diary</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Management/">Management</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Technology/">Technology</a><span class="category-list-count">29</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Security/" rel="tag">Security</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/book/" rel="tag">book</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/career/" rel="tag">career</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cloud/" rel="tag">cloud</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/deeplearning/" rel="tag">deeplearning</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/" rel="tag">math</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/med/" rel="tag">med</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mgnt/" rel="tag">mgnt</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/misc/" rel="tag">misc</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/operation/" rel="tag">operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/security/" rel="tag">security</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/tech/" rel="tag">tech</a><span class="tag-list-count">35</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Security/" style="font-size: 10px;">Security</a> <a href="/tags/book/" style="font-size: 14px;">book</a> <a href="/tags/career/" style="font-size: 14px;">career</a> <a href="/tags/cloud/" style="font-size: 12px;">cloud</a> <a href="/tags/deeplearning/" style="font-size: 18px;">deeplearning</a> <a href="/tags/math/" style="font-size: 10px;">math</a> <a href="/tags/med/" style="font-size: 12px;">med</a> <a href="/tags/mgnt/" style="font-size: 18px;">mgnt</a> <a href="/tags/misc/" style="font-size: 12px;">misc</a> <a href="/tags/operation/" style="font-size: 10px;">operation</a> <a href="/tags/python/" style="font-size: 16px;">python</a> <a href="/tags/security/" style="font-size: 10px;">security</a> <a href="/tags/tech/" style="font-size: 20px;">tech</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">January 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">August 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">May 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">April 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/10/">October 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/05/">May 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/04/">April 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/03/">March 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/12/">December 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/08/">August 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/07/">July 2015</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/06/">June 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/01/">January 2015</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/01/06/summary2020/">留给2020</a>
          </li>
        
          <li>
            <a href="/2020/01/06/summary2019/">留给2019</a>
          </li>
        
          <li>
            <a href="/2019/12/02/gcp-study/">GCP搭建serverless</a>
          </li>
        
          <li>
            <a href="/2019/01/02/aliyun-server-less/">阿里云函数计算</a>
          </li>
        
          <li>
            <a href="/2018/09/13/deep-learning-sequence-models-w3/">深度学习顺序模型第三周</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2021 Jack Wang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
	  <span id="busuanzi_container_site_uv">
	    with <span id="busuanzi_value_site_uv"></span> visitors
	  </span>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>